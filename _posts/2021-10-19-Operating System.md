---
layout: page
title:  "Operating System"
subtitle: "Computer Science of Operating System"
date:   2021-10-19 11:11:11 +0530
categories: ["CS"]
comments: true
---
## Operating System
👉 하드웨어를 관리하고, 응용프로그램과 하드웨어 사이에서 인터페이스 역할을 하며, 시스템 동작을 제어하는 시스템 소프트웨어  

<br>

## Process ✔️
👉 컴퓨터에서 연속적으로 실행되고 있는 프로그램  
👉 운영체제로부터 시스템 자원(주소 공간, 파일, 메모리 등)을 할당받는 작업의 단위  
👉 각각의 독립적인 메모리 영역(Code, Data, Heap, Stack)을 할당  
　　👉 Code  
　　　　👉 작성한 소스코드가 들어가는 텍스트 영역  
　　　　✋ 실행 파일을 구성하는 구성하는 명령어들이 올라가는 메모리 영역  
　　　　👋 함수, 제어문, 상수 등이 이곳에 지정됨  
　　👉 Data  
　　　　👉 프로그램의 시작과 동시에 할당되고, 프로그램이 종료되어야 소멸되는 영역  
　　　　👋 전역변수와 static 변수가 이곳에 할당  
　　👉 Heap  
　　　　👉 프로그래머가 할당/해제하는 메모리 영역  
　　　　👉 런 타임에 크기가 결정됨  
　　　　👋 이 공간에 메모리 할당하는 것을 동적 할당이라고 함  
　　👉 Stack  
　　　　👉 프로그램이 자동으로 사용하는 임시 메모리 영역  
　　　　👉 컴파일 타임에 크기가 결정됨  
　　　　👋 함수 호출시 생성되는 지역 변수와 매개변수가 이곳에 저장됨  
　　　　👋 함수 호출이 완료되면 사라짐  
👋 한 프로세스는 다른 프로세스의 변수나 자료구조에 접근할 수 X  
　　👋 접근하기 위해선 프로세스 간 통신(IPC) 사용  
👋 최소 한 개의 Thread를 가지고 있음  

<br>

## 프로세스의 상태
👉 생성(new)  
　　👉 갓 생성된 상태, OS 커널에 PCB 등이 생성됨  
👉 준비(ready)  
　　👉 CPU의 서비스를 받기 위해 Ready Queue에서 대기하고 있는 상태  
👉 실행(running)  
　　👉 CPU에 의해 실행되고 있는 상태  
👉 대기(waiting or blocked)  
　　👉 I/O나 다른 event를 기다리며 멈춰있는 상태  
👉 종료(terminated)  
　　👉 실행을 완료하는 등의 이유로 종료된 상태  

<br>

## 프로세스 제어 블록(PCB) ✔️
👉 특정 프로세스에 대한 중요한 정보를 저장하고 있는 운영체제의 자료구조  
　　👋 운영체제는 프로세스를 관리하기 위해 프로세스 생성과 동시에 고유한 PCB를 생성  
　　👋 프로세스 식별자, 프로세스 상태, CPU 레지스터, CPU 스케쥴링 정보, 메모리 관리 정보 등  

<br>

## 프로세스 간 통신(IPC)
👉 서로 다른 두 개의 프로세스가 정보를 주고 받는 것  
　　✋ 파이프, 소켓, 공유 메모리 등  

<br>

## Thread ✔️
👉 한 프로세스 내에서 실행되는 여러 작업 흐름의 단위  
　　👋 같은 프로세스에 속한 다른 스레드와 코드, 데이터, 힙, 열린 파일이나 신호와 같은 운영체제 자원을 공유  
　　👋 프로세스 내에서 스택과 레지스터는 따로 할당  
　　　　👋 스택은 독립적인 실행 흐름을 추가하기 위한 최소 조건  
　　　　✋ 뿐만 아니라, 이를 공유할 경우 LIFO 구조에 의해 실행 순서가 매우 복잡  
　　　　👋 레지스터는 각각 명령어의 어디까지 실행했는지를 나타내기 위해  

<br>

## Thread pool
👉 일정량의 Thread를 미리 만들어 pool에 저장하는 기법  
👋 프로그램 요청시 Thread를 할당해주고, 임무 완료시 다시 반환하여 pool에 저장  
👋 불필요하게 Thread를 생성하고 삭제하는 일이 사라져서 이와 관련된 비용을 줄여 성능 향상에 도움이 됨  
👋 But, 만들고 사용하지 않아도 계속 메모리에 할당을 해놓으므로 Memory leak 발생할 수 있음  

<br>

## 왜 멀티 프로세스 대신 멀티 스레드를 사용? ✔️
👋 프로그램을 여러 개 키는 것 보다 하나의 프로그램 안에서 여러 작업을 해결하기 위해서  
👉 프로세스를 생성하여 자원을 할당하는 시스템 콜이 줄어들어 자원을 효율적으로 관리할 수 있기 때문  
👉 또한, 프로세스 간 통신보다 스레드 간 통신 비용이 적기 때문에 멀티 프로세스 대신 멀티 스레드를 사용  
⚠️ But, 전역변수를 함께 사용하여 충돌하는 동기화 문제와 오류로 인하여 하나의 스레드가 종료 시 전체 스레드가 종료될 수 있다는 점을 주의  

<br>

## Context ✔️
👉 CPU가 다루는 해당 Task(Process or Thread)에 대한 정보  
👋 대부분의 정보는 Register에 저장되며 PCB로 관리  

<br>

## Context Switching ✔️
👉 하나의 Task가 CPU를 사용 중인 상태에서 다른 Task가 CPU를 사용하도록 하기 위해, 이전의 Task 상태(Context)를 저장하고 새로운 Task의 상태를 적재하는 작업  
👋 Process와 Thread를 처리하는 Context Switching은 조금 다름  
　　👋 Process는 OS에 의해 스케쥴링 되는 PCB를 통해 관리  
　　👋 Thread는 Process내의 TCB(Task Control Block)라는 내부 구조를 통해 관리  

<br>

## Context Switching OverHead
✋ CPU가 놀지 않도록 만들고, 사용자에게 빠르게 일처리를 제공해주기 위한 목적  
👉 CPU에 계속 프로세스를 수행시키기 위해 다른 프로세스를 실행시키고 Context Switching을 하는 것에 걸리는 시간과 메모리를 의미  

<br>

## Thread-safe ✔️
👉 멀티 스레드 환경에서 여러 스레드가 동시에 하나의 공유 자원에 접근해도 문제 없이 의도한 대로 동작하는 것  
👉 how?  
　　👉 공유 자원에 접근하는 임계 영역을 동기화 기법으로 제어해야 함  
　　👋 뮤텍스, 세마포어, 모니터  
👋 임계 구역 : 둘 이상의 스레드가 동시에 접근해서는 안 되는 공유 자원에 접근하는 코드의 일부 영역을 의미  

<br>

## 뮤텍스 🆚 세마포어 ✔️
👋 공유 자원에 여러 프로세스나 스레드가 접근하는 것을 제어하기 위한 기법  
👉 뮤텍스  
　　👉 오직 한 개의 프로세스 or 스레드만 접근할 수 있는 기법  
　　👉 동기화 대상이 오직 하나일 때 사용 가능한 기법  
　　👉 락을 획득한 프로세스만 그 락을 해제할 수 있음  
👉 세마포어  
　　👉 세마포어 변수를 설정하여 해당 변수값 만큼 프로세스 or 스레드가 접근할 수 있는 기법  
　　👉 동기화 대상이 하나 이상일 때 사용 가능한 기법  
　　👉 현재 수행중인 프로세스가 아닌 다른 프로세스가 세마포어를 해제할 수 있음  

<br>

## DeadLock(교착 상태) ✔️
👉 둘 이상의 Task들이 자원을 점유한 상태에서 서로 다른 Task가 점유하고 있는 자원을 요구하며 무한히 대기하는 상태에 빠진 현상  
👉 교착상태 4가지 조건  
　　👉 상호 배제 : 한 자원에 대한 접근 제한, 단 하나만 허용  
　　👉 점유 대기 : 자원을 점유하고 있는 Task가 접근 권한을 양보하지 않은 상태에서 다른 자원에 대한 접근 권한을 요구 가능  
　　👉 선취 불가능 : 한 Task가 다른 Task의 접근 권한을 강제로 취소할 수 없음  
　　👉 순환 대기 : 각 Task가 순환적으로 다음 Task가 요구하는 자원을 갖고 있음  
👉 교착상태 방지  
　　👉 4개의 조건 중 하나를 제거하면 됨  
　　👉 1번 조건은 제거하기 힘들며 보통 4번 조건을 제거하기 위해 초점을 맞춤  
　　👉 자원에 고유한 번호를 할당, 번호 순서대로 자원을 요구하도록 함  

<br>

## 스케쥴러 ✔️
👉 어떤 프로세스에게 자원을 할당할 지 결정하는 운영체제의 커널 모듈  
👋 모듈 : 기능이 비슷한 것들을 모아 만든 프로그램 같은 개념  

<br>

## 스케쥴링 정의와 목적 ✔️
👉 자원을 효율적으로 사용하기 위해 자원을 사용하는 순서를 결정짓는 작업  
👉 단위 시간 당 처리량을 최대로 하고 효율적으로 자원을 할당하기 위한 목적을 갖고 있음  

<br>

## 스케쥴링 단계
👉 장기 스케쥴러에 의해 어떤 프로세스가 준비 큐에 삽입될 지를 결정  
👉 단기 스케쥴러는 스케쥴링 알고리즘에 따라 CPU를 할당할 프로세스를 선택  
👉 선택된 프로세스는 running 상태가 되고 작업이 끝나면 terminated 상태가 됨  
👉 중기 스케쥴러는 메모리에 너무 많은 프로그램이 동시에 올라가는 것을 조절  
👋 준비 큐 : 현재 메모리 내에 있으면서 CPU에 의해 실행되기를 기다리는 프로세스의 집합  

<br>

## 선점형 스케쥴링
👉 프로세스가 CPU를 점유하고 있어도 우선 순위가 높은 프로세스가 오면 CPU를 빼앗을 수 있는 방식  
👋 우선 순위가 높은 프로세스가 빠르게 처리된다는 장점이 있음  
👋 잦은 Context Switching으로 Overhead가 증가하는 단점이 있음  

<br>

## 비선점형 스케쥴링
👉 프로세스가 CPU를 점유하고 있으면 다른 프로세스가 CPU를 빼앗지 못하는 방식  
👋 중간에 가로채지 못하기 때문에 응답 시간 예측이 용이하다는 장점이 있음  
👋 중요한 작업이 오래 기다릴 수 있다는 단점이 있음  

<br>

## FIFO, FCFS(First Come First Served) 스케쥴링
👉 먼저 온 프로세스 먼저 처리하는 방식  
　　👋 중요한 작업이 오래 기다릴 수 있음  

<br>

## SJF(Shortest Job First) 스케쥴링
👉 다른 프로세스가 먼저 도착했어도 CPU 요구량이 가장 적은 것부터 처리하는 방식  
　　👋 이는 최적이며, 효율성을 추구하는 것이 가장 중요하지만 특정 프로세스가 지나치게 차별받을 수 있음  
　　✋ 즉, CPU 요구량이 큰 프로세스는 거의 영원히 CPU 할당을 받지 못하는 상황이 발생할 수 있음  

<br>

## Round Robin 스케쥴링
👉 할당된 시간 안에 작업이 끝나지 않으면 해당 프로세스는 준비 큐에 들어가는 FIFO 방식  
👋 각 프로세스는 같은 크기의 CPU 시간을 할당받게 됨  
👋 스케쥴링 방식 중 시분할 시스템을 위해 설계된 선점 방식의 알고리즘  

<br>

## 기아 상태(starvation)
👉 병행 프로세스에서 프로세스가 실행되기 위한 필수적인 자원을 끊임없이 사용하지 못하는 상태  

<br>

## Swapping ✔️
👉 메모리 관리를 위해 사용되는 기법  
👉 프로세스를 불러들이기 위한 공간이 메모리에 부족하다면 현재 메모리에 적재된 프로세스들을 내보내고 (swap out), 원하는 프로세스를 불러들이는 (swap in) 방식  
👋 주 메모리 ↔️ 보조 메모리  

<br>

## 단편화 ✔️
👉 2가지의 종류  
　　👉 외부 단편화 : 메모리에 프로세스가 할당될만큼 충분히 공간이 존재하지만 공간이 쪼개져 있어 프로세스를 할당할 수 X  
　　　　👉 해결 기법 : 페이징 기법  
　　👉 내부 단편화 : 프로세스가 필요한 양보다 더 큰 메모리 공간에 할당되어 메모리가 낭비되는 것  
　　　　👉 해결 기법 : 세그멘테이션 기법  

<br>

## 페이징 기법 ✔️
👉 가상 메모리를 이용하여 프로세스를 일정한 크기인 페이지로 잘라서 메모리에 할당하는 기법  
👉 하나의 프로세스가 사용하는 메모리 공간이 연속적이어야 한다는 제약을 없애 외부 단편화 문제점을 해결할 수 있음  
👉 But, 보통 페이지 단위에 알맞게 꽉 채워 쓰는게 아니므로 내부 단편화 문제는 여전히 존재함  
✋ 또한, 그만큼 Mapping 과정 또한 늘어나기 때문에 Trade-off가 발생할 수 있음  


<br>

## 세그멘테이션 기법 ✔️
👉 가상 메모리를 이용하여 프로세스를 서로 크기가 다른 논리적인 블록 단위인 세그먼트로 분할하여 메모리에 할당하는 기법  
👉 하나의 세그먼트 단위로 통제가 가능하다는 장점  
👉 세그먼트들에 대해 필요 시 메모리에 올리고, 필요없을 경우 내리는 작업을 반복하다보면 외부 단편화가 생기는 문제점이 있음  

<br>

## 가상 메모리
👉 메모리로 실제 존재하지는 않지만 사용자에게 있어 메모리로써 역할을 하는 메모리  
👉 실제 메모리를 보조하여 프로세스 전체가 메모리 내에 올라오지 않더라도 실행이 가능하도록 하는 기법  

<br>

## Page Fault ✔️
👉 지금 실행시켜야 할 Page가 실제 메모리에 올라와 있지 않은 상황  

<br>

## Thrashing
👉 Page Fault가 과도하게 발생하는 상황  
👋 프로세스가 원활하게 수행되기 위해서는 일정 수준 이상의 페이지 프레임을 할당받아야 하는데, 너무 많은 프로세스가 적재되어 프로세스가 프레임을 충분히 할당 받지 못해 Page Fault가 발생  

<br>

## Demand Paging ✔️
👉 가상 메모리 관리 방법  
👉 프로그램 실행 시작 시, 프로그램 전체를 메모리에 적재하는 대신 초기에 필요한 부분들만 적재하는 방법  

<br>

## 캐시의 지역성 🔥
👉 캐시 메모리는 CPU의 처리 속도와 메인 메모리의 접근 속도 차이를 줄이기 위해 사용하는 고속 Buffer Memory  
👋 이러한 역할을 수행하기 위해 CPU가 어떤 데이터를 원할 것인가 어느 정도 예측해야 함  
👉 적중률이 높을수록 캐시 성능이 올라가며, 이를 높이기 위해 2가지의 데이터 지역성의 원리를 사용  
　　👉 공간 지역성  
　　　　👉 한 번 참조된 메모리 옆에 있는 메모리를 참조할 확률이 높음  
　　👉 시간 지역성  
　　　　👉 최근에 참조된 주소 내용은 곧 다음에 다시 참조됨  

<br>

## Caching line 🔥
👋 캐시는 프로세서 가까이에 위치하면서 빈번하게 사용되는 데이터를 놔두는 장소  
✋ 캐시가 아무리 가까이 있더라도 찾고자 하는 데이터가 어느 곳에 저장되어 있는지 모른다면 시간이 오래 걸리게 됨  
✋ 캐시에 목적 데이터가 저장되어 있다면 바로 접근하여 출력할 수 있어야 캐시가 의미 있음  
👉 따라서, 캐시에 데이터를 저장할 때 데이터의 메모리 주소 등을 기록해 둔 태그를 달아 특정 자료구조를 사용하여 묶음으로 저장한 것  
👉 구조 3가지  
　　👉 Direct MAP  
　　　　👉 가장 기본적인 구조, DRAM의 여러 주소가 캐시 메모리의 한 주소에 대응되는 다대일 방식  
　　👉 Full Associative  
　　　　👉 비어있는 캐시 메모리가 있으면, 마음대로 주소를 저장하는 방식  
　　👉 Set Associative  
　　　　👉 특정 행을 지정하고 그 행안의 어떤 열이든 비어있는 곳에 저장하는 방식  

<br>

## 페이지 교체 ✔️
👉 메인 메모리에 있는 특정 페이지를 내보내고 (page out), 그 자리에 필요한 다른 페이지를 올리는 것 (page in)  
👋 프로그램 실행이 계속되면 요구 페이지가 늘어나게 되고 언젠가는 메모리가 가득 차기 때문  


<br>

## 페이지 교체 알고리즘
👉 FIFO  
　　👉 먼저 물리 메모리에 들어온 페이지 순서대로 페이지 교체 시점에 먼저 교체  
👉 OPT  
　　👉 가장 오랫동안 사용되지 않을 페이지를 찾아 교체  
👉 LRU  
　　👉 가장 오랫동안 사용되지 않은 페이지를 선택하여 교체   

<br>

## Kernel ✔️
👉 하드웨어와 응용 프로그램 사이에서 인터페이스를 제공하며, 응용 프로그램이 하드웨어에서부터 오는 자원을 관리하고 사용할 수 있게 해주는 역할  

<br>

## Polling
👉 반복해서 루프를 돌며 특정 시그널이 들어왔는지 확인하는 것  

<br>

## Interrupt ✔️
👉 CPU가 프로그램을 실행하고 있을 때, 입/출력 하드웨어 등의 장치에 예외 상황이 발생하여 처리가 필요한 경우 CPU에게 알려 처리하도록 하는 것  
👋 CPU와 I/O의 속도 차이를 극복하기 위해 필요  
　　👋 I/O 연산이 CPU 명령 수행 속도보다 현저히 느리기 때문  
👉 동작 과정  
　　👉 1. CPU가 프로그램을 실행  
　　👉 2. Interrupt 요청  
　　👉 3. CPU는 프로그램을 중단하고 다음 실행할 명령어와 현재 상태를 PCB에 저장  
　　👉 4. Interrupt Service Routinue을 실행  
　　👉 5. Interrupt 종료 후 기존 상태 복구  
　　👉 6. 프로그램을 계속 실행  

<br>

## Interrupt 종류 (우선 순위)
👉 1. 외부 인터럽트 (1st)  
　　👉 하드웨어가 발생시키는 인터럽트  
　　👋 CPU 아닌 다른 하드웨어 장치가 CPU에 요청  
👉 2. 내부 인터럽트 (2nd)  
　　👉 CPU 내부에서 발생하는 인터럽트  
　　👋 Divide by Zero    
👉 3. 소프트웨어 인터럽트 (3rd)  
　　👉 소프트웨어가 발생시키는 인터럽트  
　　👋 syscall  

<br>

## syscall
👉 응용 프로그램의 요청에 따라 커널에 접근하기 위한 인터페이스  
👋 fork(), exec(), malloc() 등  
👋 인터페이스 : 동작에 도움을 주는 시스템  

<br>

## fork() 🆚 exec()
👉 한 프로세스가 다른 프로세스를 실행시키기 위해 사용  
👉 fork()  
　　👉 PID가 완전히 다른 새로운 프로세스가 생김  
　　👋 새로운 프로세스를 위한 메모리를 할당  
👉 exec()  
　　👉 PID가 새로운 프로세스에게 넘겨지며 덮어 쓰여지는 개념  
　　👋 메모리를 할당하지 않고, 새로운 프로세스만 메모리에 남음  

<br>

## 커널모드와 유저모드 구분한 이유 🔥
👉 시스템에 중요한 영향을 미치는 연산은 커널 모드에서만 실행 가능하도록 하여 하드웨어 보안을 유지하기 위함  

<br>

## 커널 수준 스레드 🆚 사용자 수준 스레드
👉 커널 수준 스레드  
　　👉 커널이 직접 스레드를 제공해주기 때문에 안정성과 다양한 기능이 제공됨  
　　👋 커널이 각 스레드를 개별적으로 관리할 수 있음  
　　👋 But, 유저/커널 간 전환이 빈번하게 이뤄지므로 성능이 저하됨  
👉 사용자 수준 스레드  
　　👉 라이브러리를 사용하기 때문에 이식성이 높고 유연한 스케쥴링 가능  
　　👋 유저/커널 모드 간 전환이 일어나지 않기 때문에 Overhead가 적음  
　　👋 But, 하나의 스레드가 블록된다면 전체 스레드가 블록됨  
👋 Overhead : 어떤 처리를 하기 위해 들어가는 간접적인 처리 시간 및 메모리  

<br>

## 캐시 🆚 레지스터
👉 캐시  
　　👉 CPU와 별도로 있는 공간, 메인 메모리와 CPU 간의 속도 차이를 극복하기 위한 것  
👉 레지스터  
　　👉 레지스터는 CPU 안에서 연산을 처리하기 위하여 데이터를 일시적으로 저장하는 공간  

<br>

## Memory Pool
👉 필요한 메모리 공간과 개수만큼 미리 만들어 pool에 저장하는 기법  
👋 프로그램 요청시 메모리를 할당해주고, 임무 완료시 다시 반환하여 pool에 저장  
👋 불필요하게 Memory를 생성하고 삭제하는 일이 사라져서 이와 관련된 비용을 줄여 성능 향상에 도움이 됨  
👋 But, 만들고 사용하지 않아도 계속 메모리에 할당을 해놓으므로 Memory leak 발생할 수 있음  

<br>
<br>

<script src="https://utteranc.es/client.js"
        repo="DCherish/DCherish.github.io"
        issue-term="pathname"
        theme="boxy-light"
        crossorigin="anonymous"
        async>
</script>